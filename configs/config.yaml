experiment_name: "egofalls_multimodal_fall_detection"

paths:
  data_root: "data/raw/egofalls"              # root folder containing all video files
  metadata_full: "data/processed/metadata_full.csv"
  metadata_subset: "data/processed/metadata_2k.csv"
  splits_dir: "data/processed/splits"
  outputs_root: "outputs"
  checkpoints_dir: "outputs/checkpoints"
  logs_dir: "outputs/logs"
  figures_dir: "outputs/figures"

reproducibility:
  seed: 42
  deterministic: true

data:
  subset_size: 350 # 12/9 was 2000              # number of clips to use (balanced falls/non-falls)
  balance_labels: true           # enforce ~50/50 fall vs non-fall when sampling subset
  label_col: "label"             # 0 = non-fall, 1 = fall
  path_col: "path"               # relative path to clip, e.g. S01/clip_0001.mp4
  subject_id_col: "subject_id"   # used for LOSO, must exist in metadata_full.csv

split:
  strategy: "random"             # "random" or "loso"
  random:
    train_frac: 0.7
    val_frac: 0.15
    test_frac: 0.15
    save_indices: true
    indices_file: "data/processed/splits/random_seed42.npz"
  loso:
    enabled: false               # set true when you implement LOSO CV loop
    subject_list_file: "data/processed/splits/loso_subject_ids.json"

video:
  enabled: true
  backbone: "resnet18"
  pretrained: true
  freeze_backbone: true
  image_size: 224                # resize shorter side to this, then center crop
  mean: [0.485, 0.456, 0.406]    # ImageNet mean
  std: [0.229, 0.224, 0.225]     # ImageNet std
  # if you want to use more than one frame later, you can extend this section

audio:
  enabled: true
  sample_rate: 16000
  target_duration_sec: 4.0       # pad/trim audio to this duration
  n_mels: 64
  n_fft: 1024
  hop_length: 256
  f_min: 0.0
  f_max: 8000.0
  log_mel: true
  normalize_per_feature: true    # optionally z-normalize mel features across dataset

model:
  mode: "fusion"                 # "fusion", "video_only", or "audio_only"
  num_classes: 2
  h_video: 128                   # hidden dim for video branch head
  h_audio: 64                    # hidden dim for audio branch head
  fusion_hidden: 128             # hidden dim in fusion classifier MLP (if multi-layer)
  dropout: 0.3
  fusion_type: "concat"          # (future-proof: "concat" vs "add" etc.)

train:
  device: "auto"                 # "auto" | "cuda" | "mps" | "cpu"
  batch_size: 16
  num_workers: 4
  max_epochs: 20
  optimizer: "adam"
  lr: 0.001
  weight_decay: 0.0001
  grad_clip_norm: 5.0
  early_stopping:
    enabled: true
    monitor: "val_f1_fall"
    mode: "max"
    patience: 5
  scheduler:
    enabled: false
    type: "step"                 # e.g., "step", "cosine"
    step_size: 5
    gamma: 0.5

eval:
  metric_primary: "f1_fall"      # F1 for fall class (label=1)
  positive_label: 1
  compute_roc_auc: true
  save_confusion_matrix: true
  confusion_matrix_file: "outputs/figures/confusion_matrix_test.png"
  save_pr_curve: false
  pr_curve_file: "outputs/figures/pr_curve_fall.png"

ablation:
  run_modes: ["fusion", "video_only", "audio_only"]
  results_file: "outputs/logs/ablation_results.csv"

logging:
  log_interval_steps: 10
  val_interval_epochs: 1
  print_model_summary: true
  train_log_file: "outputs/logs/train.log"
  save_best_on:
    metric: "val_f1_fall"
    mode: "max"
  checkpoint_names:
    fusion: "best_fusion.pt"
    video_only: "best_video_only.pt"
    audio_only: "best_audio_only.pt"
