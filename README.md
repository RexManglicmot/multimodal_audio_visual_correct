

## Question
Can we use multimodal data—(1) egocentric video from a body-worn camera and (2) synchronized ambient audio—to accurately detect whether a short clip corresponds to a fall vs non-fall event, using only a subset of 2,000 clips from the EGOFALLS dataset?


## Dataset


## Metrics



## Results



## Conclusion


## Next Steps






config.yaml ->